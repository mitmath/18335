#LyX 1.5.5 created this file. For more info see http://www.lyx.org/
\lyxformat 276
\begin_document
\begin_header
\textclass article
\begin_preamble

\renewcommand{\vec}[1]{\mathbf{#1}}

\renewcommand{\labelenumi}{(\alph{enumi})}
\renewcommand{\labelenumii}{(\roman{enumii})}

\newcommand{\tr}{\operatorname{tr}}
\end_preamble
\language english
\inputencoding auto
\font_roman times
\font_sans default
\font_typewriter default
\font_default_family default
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\paperfontsize default
\spacing single
\papersize default
\use_geometry true
\use_amsmath 2
\use_esint 0
\cite_engine basic
\use_bibtopic false
\paperorientation portrait
\topmargin 1in
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\defskip medskip
\quotes_language english
\papercolumns 1
\papersides 2
\paperpagestyle default
\tracking_changes false
\output_changes false
\author "" 
\author "" 
\end_header

\begin_body

\begin_layout Section*
18.335 Problem Set 5 Solutions
\end_layout

\begin_layout Subsection*
Problem 1 (10+10+10 pts): 
\end_layout

\begin_layout Standard
The results for parts (a) and (b) are shown in figure\InsetSpace ~

\begin_inset LatexCommand ref
reference "fig:prob2"

\end_inset

.
 Both Lanczos and restarted Lanczos converge to more than six significant
 digits to the minimal 
\begin_inset Formula $\lambda$
\end_inset

 in under 50 iterations, but the restarted version has much more erratic
 convergence.
\end_layout

\begin_layout Standard
My Matlab code (very inefficient, but does the job) is given below.
 Note that, in order to restart, we must keep track of the 
\begin_inset Formula $Q_{n}$
\end_inset

 matrix (save the 
\begin_inset Formula $q$
\end_inset

 vectors) in order to compute the Ritz vector 
\begin_inset Formula $Q_{n}y$
\end_inset

 from an eigenvector 
\begin_inset Formula $y$
\end_inset

 of 
\begin_inset Formula $T_{n}$
\end_inset

.
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Standard
\align center
\begin_inset Graphics
	filename prob2.eps
	width 70col%

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Caption

\begin_layout Standard
\begin_inset LatexCommand label
name "fig:prob2"

\end_inset

Fractional error in minimum eigenvalue 
\begin_inset Formula $\lambda$
\end_inset

 as a function of iterations (matrix-vector multiples) for Lanczos and for
 restarted Lanczos (restarting with the best Ritz vector every 10 iterations).
\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Include \verbatiminput{lanczos-sol.m}
preview false

\end_inset


\end_layout

\begin_layout Standard
In part (c), I asked you to repeat the same process, except selecting the
 min-
\begin_inset Formula $|\lambda|$
\end_inset

 eigenvalue at each step.
 For this particular 
\begin_inset Formula $A$
\end_inset

, the smalles two eigenvalues are 
\begin_inset Formula $-0.3010$
\end_inset

 and 
\begin_inset Formula $+0.3828$
\end_inset

, so the minimum-
\begin_inset Formula $|\lambda|$
\end_inset

 eigenvalue is the same as the minimum-
\begin_inset Formula $\lambda$
\end_inset

 eigenvalue, so at first glance you might expect the same results as before.
 However, the algorithm doesn't 
\begin_inset Quotes eld
\end_inset

know
\begin_inset Quotes erd
\end_inset

 this fact, and Lanczos is much better at getting extremal (min/max) eigenvalues
 correct than eigenvalues in the interior of the spectrum.
 The results are shown in figure\InsetSpace ~

\begin_inset LatexCommand ref
reference "fig:prob2c"

\end_inset

.
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Standard

\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename prob2c.eps
	width 70col%

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Caption

\begin_layout Standard
\begin_inset LatexCommand label
name "fig:prob2c"

\end_inset

Fractional error in minimum-
\begin_inset Formula $|\lambda|$
\end_inset

 eigenvalue 
\begin_inset Formula $\lambda$
\end_inset

 as a function of iterations (matrix-vector multiples) for Lanczos and for
 restarted Lanczos (restarting with the best Ritz vector every 10 iterations).
 The convergence is irregular because Lanczos has trouble getting the interior
 of the spectrum accurately.
\end_layout

\end_inset


\end_layout

\begin_layout Standard

\end_layout

\end_inset

 The nonrestarted case appears to converge the same as before at first,
 but periodically a spurious/ghost eigenvalue appears near 
\begin_inset Formula $0$
\end_inset

 that messes up convergence until it goes away.
 The restarted case is even worse.
 (The situation would be even worse if we tried to get an eigenvalue even
 further in the interior of the spectrum, e.g.
 the eigenvalue closest to 2.)
\end_layout

\begin_layout Subsection*
Problem 2 (25 pts): 
\end_layout

\begin_layout Standard
The residual curves are plotted in figure\InsetSpace ~

\begin_inset LatexCommand ref
reference "fig:prob3"

\end_inset

.
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Standard

\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename prob3.eps
	width 70col%

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Caption

\begin_layout Standard
\begin_inset LatexCommand label
name "fig:prob3"

\end_inset

Convergence of the residual for problem 38.6, using conjugate-gradient (CG)
 or steepest-descent (SD), and also the actual CG residual 
\begin_inset Formula $\Vert Ax-b\Vert$
\end_inset

 as well as the estimated upper bound on the residual from theorem 38.5.
\end_layout

\end_inset


\end_layout

\begin_layout Standard

\end_layout

\end_inset

 Several things are apparent.
 First, as expected, CG (conjugate gradient) converges much faster than
 SD (steepest descent).
 Since the condition number 
\begin_inset Formula $\kappa(A)\approx397$
\end_inset

 for this 
\begin_inset Formula $A$
\end_inset

, we expect 
\begin_inset Formula $SD$
\end_inset

 to take several hundred iterations to converge, while 
\begin_inset Formula $CG$
\end_inset

 should take only several dozen.
 Second the computed residual for CG matches the actual residual until we
 hit machine precision (
\begin_inset Formula $\approx10^{-16}$
\end_inset

), at which point rounding errors prevent the actual residual from decreasing
 further (while the computed residual keeps going down).
 Third, the estimate 
\begin_inset Formula $2(\sqrt{\kappa}-1)^{n}(\sqrt{\kappa}+1)^{n}$
\end_inset

 from theorem 38.5 is grossly pessimistic.
\end_layout

\begin_layout Standard
What is happening, as discussed in class, is that the CG process is effectively
 shrinking the size of the vector space as it goes along, because once it
 searches a direction it never needs to search that direction again, and
 the condition number of 
\begin_inset Formula $A$
\end_inset

 restricted to the remaining subspace gets better and better---we get 
\emph on
superlinear convergence
\emph default
, observed in the fact that the residual on decreases faster than exponentially
 (which would be a straight line on a semilog scale).
\end_layout

\begin_layout Standard
We can quantify this in various ways.
 For example, since on each step the residual is 
\begin_inset Quotes eld
\end_inset

supposed
\begin_inset Quotes erd
\end_inset

 to decrease by a factor 
\begin_inset Formula $f=(\sqrt{\kappa}-1)/(\sqrt{\kappa}+1)$
\end_inset

, we can define an 
\begin_inset Quotes eld
\end_inset

effective
\begin_inset Quotes erd
\end_inset

 condition number 
\begin_inset Formula $\tilde{\kappa}_{n}$
\end_inset

 from the actual rate of decrease 
\begin_inset Formula $f_{n}=(\sqrt{\tilde{\kappa}_{n}}-1)/(\sqrt{\tilde{\kappa}_{n}}+1)=\Vert r_{n+1}\Vert/\Vert r_{n}\Vert$
\end_inset

.
 Thus, 
\begin_inset Formula $\sqrt{\tilde{\kappa}_{n}}=(1+f_{n})/(1-f_{n})$
\end_inset

.
 By the time 30 iterations have passed, 
\begin_inset Formula $\tilde{\kappa}_{30}$
\end_inset

 is only 38; by 40 iterations, 
\begin_inset Formula $\tilde{\kappa}_{40}\approx21$
\end_inset

; by 
\begin_inset Formula $60$
\end_inset

 iterations, 
\begin_inset Formula $\tilde{\kappa}_{60}\approx8.8$
\end_inset

.
 If we look at the set of eigenvalues, we see that eliminating just the
 two smallest 
\begin_inset Formula $\lambda$
\end_inset

 would give a condition number of 34, and eliminating the 11 smallest eigenvalue
s would give a condition number of 8.4.
 (Eliminating the largest eigenvalues does not reduce the condition number
 nearly as quickly.) So, what seems to be happening is that CG is effectively
 eliminating the smallest eigenvalues from the spectrum as the algorithm
 progresses, improving the condition number of 
\begin_inset Formula $A$
\end_inset

 and hence the convergence rate is better than the pessimistic bound of
 theorem 38.5.
\end_layout

\begin_layout Standard
The code for SD was posted on the web site; my code for CG in this problem
 is listed below (modified to add an additional stopping criteria for problem
 3).
\begin_inset Include \verbatiminput{CG.m}
preview false

\end_inset


\end_layout

\begin_layout Subsection*
Problem 3 (15 points)
\end_layout

\begin_layout Standard
The results for this problem are shown in problem\InsetSpace ~
xxx.
 They are a bit odd, in that the conjugate-gradient method indeed converges
 to a null-space vector, but when the residual 
\begin_inset Formula $\Vert Ax\Vert$
\end_inset

 gets very small (
\begin_inset Formula $\approx10^{-10}$
\end_inset

), the iteration goes crazy and the residual norm shoots up again.
 This seems to be due to some sort of roundoff disaster.
 In practice, one could simply stop the iterations once the residual became
 sufficiently small.
 There may also be ways of mitigating the roundoff problems; for example,
 in blue are shown the results if CG is simply restarted once the residual
 becomes small, in which case the residual continues to decrease for another
 couple orders of magnitude before errors resurface, and there are probably
 more clever corrections as well.
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Standard

\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename prob3new.eps
	width 70col%

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Caption

\begin_layout Standard
\begin_inset LatexCommand label
name "fig:prob3new"

\end_inset

Convergence of conjugate gradient to a null-space vector of a positive 
\emph on
semi
\emph default
-definite matrix 
\begin_inset Formula $A$
\end_inset

.
 The convergence breaks down once the residual 
\begin_inset Formula $\Vert Ax\Vert$
\end_inset

 becomes very small, probably because roundoff errors effectively give 
\begin_inset Formula $A$
\end_inset

 a small negative eigenvalue.
 Simply stopping CG when the residual becomes small and restarting from
 there (blue line) improves matters somewhat, allowing the residual to be
 decreased by another couple of orders of magnitude before a breakdown resurface
s.
\end_layout

\end_inset


\end_layout

\begin_layout Standard

\end_layout

\end_inset


\end_layout

\begin_layout Standard
What is the source of the roundoff disaster? I haven't tried to diagnose
 it in detail, but a simple check of the eigenvalues of 
\begin_inset Formula $A$
\end_inset

 reveals a clue.
 Although 
\begin_inset Formula $A$
\end_inset

 should, in principle, be positive semidefinite, in practice because of
 roundoff errors I find that it could have at least one slightly negative
 eigenvalue.
 Once roundoff errors have made the problem indefinite, the well-known possibili
ty of breakdown of the conjugate-gradient algorithm resurfaces.
\begin_inset Foot
status collapsed

\begin_layout Standard
See, e.g., Paige, Parlett, and Van der Vorst, 
\begin_inset Quotes eld
\end_inset

Approximate solutions and eigenvalue bounds from Krylov subspaces,
\begin_inset Quotes erd
\end_inset

 
\emph on
Numer.
 Lin.
 Alg.
 Appls.

\emph default
 
\series bold
2
\series default
(2), 115--133 (1995), or Fischer, Hanke, and Hochbruck, 
\begin_inset Quotes eld
\end_inset

A note on conjugate-gradient type methods for indefinite and/or inconsistent
 linear systems,
\begin_inset Quotes erd
\end_inset

 
\emph on
Num.
 Algorithms
\emph default
 
\series bold
11
\series default
, 181--187 (1996).
 
\end_layout

\end_inset


\end_layout

\end_body
\end_document
