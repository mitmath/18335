#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\begin_preamble
\newcommand{\tr}{\operatorname{tr}}
\end_preamble
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 2
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Quasi-Newton optimization:
\begin_inset Newline newline
\end_inset

Origin of the BFGS update
\end_layout

\begin_layout Author
Steven G.
 Johnson, notes for 18.335 at MIT
\end_layout

\begin_layout Abstract
In a typical optimization setting we are provided with an objective function
 
\begin_inset Formula $f(x)$
\end_inset

 and its gradient 
\begin_inset Formula $\nabla f$
\end_inset

 only.
 However, as these are evaluated for many different points 
\begin_inset Formula $x$
\end_inset

 we can infer something about the second derivative (
\begin_inset Quotes eld
\end_inset

Hessian
\begin_inset Quotes erd
\end_inset

) by watching how 
\begin_inset Formula $\nabla f$
\end_inset

 changes, and by incorporating that information into our optimization algorithm
 we can accelerate convergence.
 This approach leads to 
\begin_inset Quotes eld
\end_inset

quasi-Newton
\begin_inset Quotes erd
\end_inset

 or 
\begin_inset Quotes eld
\end_inset

variable-metric
\begin_inset Quotes erd
\end_inset

 methods, so-called because they approximate an exact Newton step for 
\begin_inset Formula $\nabla f=0$
\end_inset

.
 The most widely used method to approximate the Hessian is a BFGS update,
 and in these notes we survey the basic ideas underlying this important
 algorithm.
\end_layout

\begin_layout Section
Newton steps and backtracking
\end_layout

\begin_layout Standard
Suppose that we are trying to solve 
\begin_inset Formula 
\[
\min_{x\in\mathbb{R}^{n}}f(x)
\]

\end_inset

and we are supplied a method to efficiently compute both 
\begin_inset Formula $f(x)$
\end_inset

 and 
\begin_inset Formula $\nabla f$
\end_inset

 (e.g.
 by an adjoint method).
\end_layout

\begin_layout Standard
On step 
\begin_inset Formula $k$
\end_inset

 of optimization, let 
\begin_inset Formula $x^{k}$
\end_inset

 be our current iterate, and let 
\begin_inset Formula $g^{k}=\left.\nabla f\right|_{x^{k}}$
\end_inset

.
 If we had the 
\series bold
second derivative 
\begin_inset Quotes eld
\end_inset

Hessian
\begin_inset Quotes erd
\end_inset

 matrix
\series default
 
\begin_inset Formula $H^{k}$
\end_inset

 as well (
\begin_inset Formula $H_{ij}^{k}=\left.\frac{\partial f}{\partial x_{i}\partial x_{j}}\right|_{x^{k}}$
\end_inset

), then we could try to make progress via second-order (quadratic) Taylor
 expansion 
\begin_inset Formula 
\[
f(x^{k}+\delta)\approx f(x^{k})+\delta^{T}g^{k}+\frac{1}{2}\delta^{T}H^{k}\delta=q(\delta).
\]

\end_inset

Near a local minimum, 
\begin_inset Formula $H$
\end_inset

 is positive-definite, and the exact minimum is 
\begin_inset Formula 
\[
\delta^{k}=-(H^{k})^{-1}g^{k}.
\]

\end_inset

In fact, this is exactly a 
\series bold
Newton step
\series default
 in finding a root of 
\begin_inset Formula $\nabla f=0$
\end_inset

, where we approximate the gradient near 
\begin_inset Formula $x$
\end_inset

 by a 
\emph on
first-order
\emph default
 Taylor expansion 
\begin_inset Formula 
\[
\left.\nabla f\right|_{x+\delta}\approx g^{k}+H^{k}\delta.
\]

\end_inset

However, we might run into a problem: the Newton step 
\begin_inset Formula $\delta$
\end_inset

 might be so large that our Taylor expansion is not accurate, and 
\begin_inset Formula $f(x^{k}+\delta)$
\end_inset

 might actually get 
\emph on
worse
\emph default
.
 There are a couple of common approaches to fix this:
\end_layout

\begin_layout Enumerate

\series bold
Trust region
\series default
: minimize a 
\begin_inset Formula $q(\delta)$
\end_inset

 only for 
\begin_inset Formula $\delta$
\end_inset

 sufficiently small, i.e.
 in a 
\begin_inset Quotes eld
\end_inset

trust region.
\begin_inset Quotes erd
\end_inset

 For example, a common choice is a spherical trust region 
\begin_inset Formula $\Vert\delta\Vert_{2}<r^{k}$
\end_inset

 for some radius 
\begin_inset Formula $r^{k}$
\end_inset

, in which case there is a nice result: strong duality holds, and we can
 optimize a convex dual problem.
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
This is called the 
\begin_inset Quotes eld
\end_inset

trust region problem,
\begin_inset Quotes erd
\end_inset

 and is discussed in e.g.
 Boyd & Vandenberghe section 5.2.
\end_layout

\end_inset

 If the resulting step is not 
\begin_inset Quotes eld
\end_inset

acceptable
\begin_inset Quotes erd
\end_inset

 (see below), we can change the trust-region radius and try again.
\end_layout

\begin_layout Enumerate

\series bold
Line search
\series default
: We can minimize 
\begin_inset Formula $f(x^{k}+\alpha\delta^{k})$
\end_inset

 over 
\begin_inset Formula $\alpha\in\mathbb{R}$
\end_inset

, i.e.
 along the direction of the Newton step
\begin_inset space ~
\end_inset


\begin_inset Formula $\delta^{k}$
\end_inset

.
 Usually minimizing this 
\emph on
exactly
\emph default
 is more trouble than it is worth just to take a single optimization step,
 so it is common to do an 
\series bold
inexact line search
\series default
: try different 
\begin_inset Formula $\alpha$
\end_inset

 until the result is 
\begin_inset Quotes eld
\end_inset

acceptable
\begin_inset Quotes erd
\end_inset

 (see below).
 
\end_layout

\begin_layout Enumerate

\series bold
Backtracking
\series default
: Instead of exact line search, we try 
\begin_inset Formula $f(x^{k}+\alpha\delta^{k})$
\end_inset

 for 
\begin_inset Formula $\alpha=1,\tau,\tau^{2},\tau^{3},\ldots$
\end_inset

 where 
\begin_inset Formula $0<\tau<1$
\end_inset

 is some parameter (e.g.
 
\begin_inset Formula $\tau=0.5$
\end_inset

), until the result is 
\begin_inset Quotes eld
\end_inset

acceptable
\begin_inset Quotes erd
\end_inset

 (see below).
\end_layout

\begin_layout Standard
For both a trust region and backtracking line search we have to decide whether
 a given step 
\begin_inset Formula $\delta$
\end_inset

 is acceptable.
 Naively, we can simply check whether 
\begin_inset Formula $f(x^{k}+\delta)<f(x^{k})$
\end_inset

, but in practice it turns out that we want to impose stronger conditions
 — we only want to take steps 
\begin_inset Formula $\delta$
\end_inset

 where our quadratic approximation 
\begin_inset Formula $q(\delta)$
\end_inset

 is reasonably accurate.
 In practice, we typically impose one or both of the 
\series bold
Wolfe conditions
\series default
 on the step 
\begin_inset Formula $\delta$
\end_inset

: 
\end_layout

\begin_layout Enumerate
\begin_inset Formula $f(x^{k}+\delta)\leq f(x^{k})+c_{1}\delta^{T}g^{k}$
\end_inset

 where 
\begin_inset Formula $0<c_{1}<1$
\end_inset

, typically 
\begin_inset Formula $c_{1}=10^{-4}$
\end_inset

 : 
\begin_inset Formula $f$
\end_inset

 must decrease at least proportional to the prediction of the gradient 
\begin_inset Formula $g^{k}$
\end_inset

.
\end_layout

\begin_layout Enumerate
\begin_inset Formula $|\delta^{T}g^{k+1}|\leq c_{2}|\delta^{T}g^{k}|$
\end_inset

, where 
\begin_inset Formula $g^{k+1}=\left.\nabla f\right|_{x^{k}+\delta}$
\end_inset

 and 
\begin_inset Formula $0<c_{2}<1$
\end_inset

, e.g.
 
\begin_inset Formula $c_{2}=0.9$
\end_inset

: the derivative 
\begin_inset Formula $\delta^{T}\nabla f$
\end_inset

 along the search direction must decrease sufficiently.
 (Note that for an exact line search we will have 
\begin_inset Formula $g^{k+1}=0$
\end_inset

.) This condition helps prevent trust-region or inexact line-search methods
 from taking steps 
\begin_inset Formula $\delta$
\end_inset

 that are too small, and it also leads to a nice property of BFGS updates
 below.
\end_layout

\begin_layout Section
Quasi-Newton/Variable-metric methods
\end_layout

\begin_layout Standard
The problem with Newton steps is that the exact Hessian is hard to come
 by when 
\begin_inset Formula $n$
\end_inset

 is large.
 Even with adjoint methods, evaluating 
\begin_inset Formula $H$
\end_inset

 exactly typically costs at least 
\begin_inset Formula $n$
\end_inset

 times the cost of evaluating 
\begin_inset Formula $f$
\end_inset

 once (it correspond to taking the gradient 
\begin_inset Formula $n$
\end_inset

 times: one more gradient for each component of 
\begin_inset Formula $\nabla f$
\end_inset

).
 When 
\begin_inset Formula $n$
\end_inset

 is really large, just 
\emph on
storing
\emph default
 the 
\begin_inset Formula $H$
\end_inset

 matrix (
\begin_inset Formula $n^{2}$
\end_inset

 numbers) might be impractical.
 Instead, 
\begin_inset Quotes eld
\end_inset

quasi
\begin_inset Quotes erd
\end_inset

 Newton methods (also called 
\begin_inset Quotes eld
\end_inset

variable-metric
\begin_inset Quotes erd
\end_inset

 methods) apply the same Newton steps above but use an 
\series bold
approximate Hessian
\series default
 
\begin_inset Formula $H^{k}$
\end_inset

, often a low-rank approximation (which can be stored and applied efficiently).
 In fact, since what is needed for the Newton step is 
\begin_inset Formula $(H^{k})^{-1}$
\end_inset

, usually one stores a low-rank 
\series bold
approximate inverse Hessian
\series default
.
 To obtain this, we want to 
\series bold
iteratively
\series default
 construct our approximate 
\begin_inset Formula $H^{k+1}$
\end_inset

 (or 
\begin_inset Formula $(H^{k+1})^{-1}$
\end_inset

) 
\series bold
given only the gradient
\series default
 (
\emph on
first
\emph default
 derivative) of 
\begin_inset Formula $f$
\end_inset

.
 Some desired properties of 
\begin_inset Formula $H^{k}$
\end_inset

 are:
\end_layout

\begin_layout Enumerate
For a convex quadratic 
\begin_inset Formula $f(x)$
\end_inset

, 
\begin_inset Formula $H^{k}$
\end_inset

 should approach the exact Hessian as 
\begin_inset Formula $k\to\infty$
\end_inset

 (i.e., as we apply our iterative update for many points and many gradient
 evaluations, approaching the minimum).
 In practice, what can typically be proved
\begin_inset space ~
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "Broyden67,FletcherPowell63,Goldfarb70"
literal "false"

\end_inset

 is that for a convex quadratic 
\begin_inset Formula $f(x)$
\end_inset

, the quasi-Newton method gives the exact minimum and the exact Hessian
 in 
\begin_inset Formula $n$
\end_inset

 steps (in exact arithmetic).
\end_layout

\begin_layout Enumerate

\series bold
Secant condition
\series default
: 
\begin_inset Formula 
\[
\underbrace{g^{k+1}-g^{k}}_{\gamma}=H^{k+1}\underbrace{(x^{k+1}-x^{k})}_{\delta}.
\]

\end_inset

This condition arises because it would be true of the 
\emph on
exact
\emph default
 Hessian for a quadratic 
\begin_inset Formula $f$
\end_inset

 (see the 
\begin_inset Formula $\left.\nabla f\right|_{x+\delta}$
\end_inset

 Taylor expansion above).
 Equivalently, 
\begin_inset Formula $H^{k}$
\end_inset

 must at least predict the change in the gradient on the 
\begin_inset Formula $k$
\end_inset

-th step.
\end_layout

\begin_layout Enumerate
Real-symmetric positive-definite.
 This makes our 
\begin_inset Formula $q(n)$
\end_inset

 function convex and 
\begin_inset Formula $\delta^{k}=-(H^{k})^{-1}g^{k}$
\end_inset

 is in the 
\begin_inset Quotes eld
\end_inset

downhill
\begin_inset Quotes erd
\end_inset

 direction from 
\begin_inset Formula $x^{k}$
\end_inset

.
\end_layout

\begin_layout Enumerate
\begin_inset Formula $H^{k}$
\end_inset

 should 
\begin_inset Quotes eld
\end_inset

remember
\begin_inset Quotes erd
\end_inset

 as much information from previous steps (i.e.
 the previous gradient evaluations) as possible.
 (We 
\emph on
don't
\emph default
 want to impose the secant conditions on all steps simultaneously, however,
 because this could quickly become impossible: 
\begin_inset Formula $f$
\end_inset

 may not be exactly quadratic.)
\end_layout

\begin_layout Standard
The last criterion is rather vague and could lead to many possible quasi-Newton
 algorithms.
 However, it turns out that an extremely easy and powerful approach to 
\begin_inset Quotes eld
\end_inset

remembing
\begin_inset Quotes erd
\end_inset

 information is to simply 
\series bold
minimize the change in 
\series default

\begin_inset Formula $H^{k}$
\end_inset

: we minimize 
\begin_inset Formula $\Vert H^{k+1}-H^{k}\Vert$
\end_inset

 in some norm, or alternatively minimize 
\begin_inset Formula $\Vert(H^{k+1})^{-1}-(H^{k})^{-1}\Vert$
\end_inset

.
 In the appropriate choice of norm, the latter leads to the famous 
\begin_inset Quotes eld
\end_inset

BFGS
\begin_inset Quotes erd
\end_inset

 update, which has lots of nice properties.
\end_layout

\begin_layout Section
BFGS updates
\end_layout

\begin_layout Standard
This update, named for 
\series bold
B
\series default
royden
\begin_inset space ~
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "Broyden70"
literal "false"

\end_inset

, 
\series bold
F
\series default
letcher
\begin_inset space ~
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "Fletcher70"
literal "false"

\end_inset

, 
\series bold
G
\series default
oldfarb
\begin_inset space ~
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "Goldfarb70"
literal "false"

\end_inset

, and 
\series bold
S
\series default
hanno
\begin_inset space ~
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "Shanno70"
literal "false"

\end_inset

, who wrote four 
\emph on
separate
\emph default
 papers that developed the approach in 1970, is obtained by solving 
\begin_inset Formula 
\[
\min_{H\in\mathbb{R}^{n\times n}}\Vert H^{-1}-(H^{k})^{-1}\Vert_{W}
\]

\end_inset


\begin_inset Formula 
\[
\text{subject to }H^{-1}\gamma=\delta\text{ and }H^{T}=H
\]

\end_inset

That is, we minimize the change in 
\begin_inset Formula $H^{-1}$
\end_inset

 subject to the second condition and require that it be real-symmetric (it
 will turn out that we get positive-definiteness 
\begin_inset Quotes eld
\end_inset

for free
\begin_inset Quotes erd
\end_inset

 below).
 Here, 
\begin_inset Formula $\Vert\cdots\Vert_{W}$
\end_inset

 is a weighted Frobenius norm 
\begin_inset Formula 
\[
\Vert A\Vert_{W}^{2}=\frac{1}{2}\tr\left[WAWA^{T}\right]=\frac{1}{2}\Vert MAM^{T}\Vert_{F}^{2}=\frac{1}{2}\tr\left[MAM^{T}MA^{T}M^{T}\right]
\]

\end_inset

where 
\begin_inset Formula $W=M^{T}M$
\end_inset

 is a positive-definite 
\begin_inset Quotes eld
\end_inset

weight
\begin_inset Quotes erd
\end_inset

 matrix to be chosen later (recall the identity 
\begin_inset Formula $\tr AB=\tr BA$
\end_inset

).
 If we let 
\begin_inset Formula $E=H^{-1}-(H^{k})^{-1}$
\end_inset

, require that the previous iterate 
\begin_inset Formula $H^{k}$
\end_inset

 be symmetric, then this optimization problem equivalently becomes 
\begin_inset Formula 
\[
\min_{E\in\mathbb{R}^{n\times n}}\Vert E\Vert_{W}^{2}
\]

\end_inset


\begin_inset Formula 
\[
\text{subject to }Ey=r\text{ and }E^{T}=E
\]

\end_inset

 where 
\begin_inset Formula $y=\gamma$
\end_inset

 and 
\begin_inset Formula $r=\delta-(H^{k})^{-1}\gamma$
\end_inset

.
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
If alternatively we were minimizing 
\begin_inset Formula $\Vert H-H^{k}\Vert_{W}$
\end_inset

, we would get exactly the same form of minimization problem with 
\begin_inset Formula $E=H-H^{k}$
\end_inset

, 
\begin_inset Formula $y=\delta$
\end_inset

, and 
\begin_inset Formula $r=\gamma-H^{k}\delta$
\end_inset

.
 This leads to an alternative quasi-Newton method, called the Davidon–Fletcher–P
owell (DFP) method, that seems not to perform quite as well in practice.
 Intuitively, since 
\begin_inset Formula $H^{-1}$
\end_inset

 is the quantity that appears in the Newton step, it is not too surprising
 that it is better to minimize the change in 
\begin_inset Formula $H^{-1}$
\end_inset

 rather than the change in 
\begin_inset Formula $H$
\end_inset

.
\end_layout

\end_inset

 This optimization problem is, in fact, a 
\series bold
QP
\series default
: we are minimizing a convex quadratic objective subject to affine constraints.
 In consequence, strong duality holds and we can instead solve the Lagrange
 dual problem.
 Equivalently, we can solve the KKT conditions.
 It turns out that this leads to a very nice formula for the update 
\begin_inset Formula $E$
\end_inset

 if we make the right choice of weight matrix 
\begin_inset Formula $W$
\end_inset

.
\end_layout

\begin_layout Standard
Let's work out the Lagrange dual problem, following Greenstadt
\begin_inset space ~
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "Greenstadt70"
literal "false"

\end_inset

 and Goldfarb
\begin_inset space ~
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "Goldfarb70"
literal "false"

\end_inset

.
 We define Lagrange multipliers 
\begin_inset Formula $\lambda\in\mathbb{R}^{n}$
\end_inset

 for the 
\begin_inset Formula $Ey-r=0$
\end_inset

 constraint and 
\begin_inset Formula $\Gamma^{T}\in\mathbb{R}^{n\times n}$
\end_inset

 for the 
\begin_inset Formula $E-E^{T}=0$
\end_inset

 constraint, and obtain the Lagrangian
\begin_inset Formula 
\[
L(E,\lambda,\Gamma)=\tr\left[\frac{1}{2}WEWE^{T}+(Ey-r)\lambda^{T}+\Gamma(E-E^{T})\right].
\]

\end_inset

Here, note that 
\begin_inset Formula $\tr\left[(Ey-r)\lambda^{T}\right]=\tr\left[\lambda^{T}(Ey-r)\right]=\lambda^{T}(Ey-r)$
\end_inset

 is just the ordinary sum of 
\begin_inset Formula $n$
\end_inset

 Lagrange multipliers 
\begin_inset Formula $\lambda_{i}$
\end_inset

 times 
\begin_inset Formula $n$
\end_inset

 constraints, but by re-ordering it into a rank-1 matrix we were able to
 combine it with the 
\begin_inset Formula $\Vert E\Vert_{W}^{2}$
\end_inset

 trace.
 And 
\begin_inset Formula $\tr\left[\Gamma(E-E^{T})\right]=\sum_{i,j}\Gamma_{ji}(E-E^{T})_{ji}=\sum_{i,j}(\Gamma^{T})_{ij}(E-E^{T})_{ij}$
\end_inset

 is a 
\begin_inset Quotes eld
\end_inset

Frobenius inner product
\begin_inset Quotes erd
\end_inset

 of the 
\begin_inset Formula $n^{2}$
\end_inset

 Lagrange multipliers 
\begin_inset Formula $(\Gamma^{T})_{ij}$
\end_inset

 with the 
\begin_inset Formula $n^{2}$
\end_inset

 constraints from 
\begin_inset Formula $E^{T}=E$
\end_inset

.
 Note that 
\begin_inset Formula 
\[
\nabla_{B}\tr(BC)=\nabla_{B}\sum_{ij}B_{ij}C_{ji}=C^{T},
\]

\end_inset

where 
\begin_inset Formula $\nabla_{B}$
\end_inset

 denotes the matrix of partial derivatives 
\begin_inset Formula $\frac{\partial\tr(BC)}{\partial B_{ij}}=C_{ji}$
\end_inset

, and similarly 
\begin_inset Formula $\nabla_{B}\tr(B^{T}C)=C$
\end_inset

.
 We can now solve the KKT conditions
\begin_inset Formula 
\begin{align*}
\nabla_{E}L & =0=WEW+\lambda y^{T}+\Gamma^{T}-\Gamma\\
Ey-r & =0\\
E^{T}-E & =0.
\end{align*}

\end_inset

subject to the constraints 
\begin_inset Formula $Ey=r$
\end_inset

 and 
\begin_inset Formula $E^{T}=E$
\end_inset

.
 The first equation gives 
\begin_inset Formula 
\[
E=-W^{-1}\left(\lambda y^{T}+\Gamma^{T}-\Gamma\right)W^{-1}.
\]

\end_inset

The requirement that 
\begin_inset Formula $E=E^{T}$
\end_inset

 then means that 
\begin_inset Formula $\left(\lambda y^{T}+\Gamma^{T}-\Gamma\right)=\left(\lambda y^{T}+\Gamma^{T}-\Gamma\right)^{T}$
\end_inset

, or equivalently 
\begin_inset Formula 
\[
\Gamma^{T}-\Gamma=\frac{1}{2}\left(y\lambda^{T}-\lambda y^{T}\right)
\]

\end_inset

and hence 
\begin_inset Formula 
\[
E=-\frac{1}{2}W^{-1}\left(y\lambda^{T}+\lambda y^{T}\right)W^{-1}.
\]

\end_inset

Finally, the condition 
\begin_inset Formula $Ey=r$
\end_inset

 now implies 
\begin_inset Formula 
\[
y\lambda^{T}W^{-1}y+\lambda\left(y^{T}W^{-1}y\right)=-2Wr.
\]

\end_inset

Since the 
\begin_inset Formula $(\cdots)$
\end_inset

 term is a scalar, we can solve for 
\begin_inset Formula 
\[
\lambda=-\frac{2Wr+y\lambda^{T}W^{-1}y}{y^{T}W^{-1}y}.
\]

\end_inset

At first glance, this doesn't seem immediately helpful since there is a
 
\begin_inset Formula $\lambda^{T}$
\end_inset

 on the right hand side.
 But if we multiply both sides by 
\begin_inset Formula $y^{T}W^{-1}$
\end_inset

 and transpose, we can solve for the unknown scalar 
\begin_inset Formula $\lambda^{T}W^{-1}y$
\end_inset

: 
\begin_inset Formula 
\[
\lambda^{T}W^{-1}y=-\frac{2r^{T}y+y^{T}W^{-1}y\left(\lambda^{T}W^{-1}y\right)}{y^{T}W^{-1}y}\implies\lambda^{T}W^{-1}y=-\frac{r^{T}y}{y^{T}W^{-1}y}.
\]

\end_inset

Plugging this back into 
\begin_inset Formula $\lambda=\cdots,$
\end_inset

 we get 
\begin_inset Formula 
\[
\lambda=-\frac{2Wr+-\frac{yr^{T}y}{y^{T}W^{-1}y}}{y^{T}W^{-1}y}=\frac{yy^{T}r}{\left(y^{T}W^{-1}y\right)^{2}}-\frac{2Wr}{y^{T}W^{-1}y}.
\]

\end_inset

Finally, we can substitute this into our 
\begin_inset Formula $E$
\end_inset

 equation to obtain
\begin_inset Formula 
\[
E=\frac{1}{y^{T}W^{-1}y}\left[ry^{T}W^{-1}+W^{-1}yr^{T}-\frac{y^{T}r}{y^{T}W^{-1}y}W^{-1}yy^{T}W^{-1}\right].
\]

\end_inset

This looks messy, but it is actually quite nice: a 
\series bold
sum of rank-1 updates
\series default
 to the inverse Hessian! But we have one trick left up our sleeve: we haven't
 chosen our weight 
\begin_inset Formula $W$
\end_inset

 yet! Different choices of 
\begin_inset Formula $W$
\end_inset

 lead to different quasi-Newton methods, but it is useful to note that 
\begin_inset Formula $E$
\end_inset

 only involves 
\begin_inset Formula $W$
\end_inset

 via the combination 
\begin_inset Formula $W^{-1}y$
\end_inset

.
\end_layout

\begin_layout Standard
To get an 
\begin_inset Formula $E$
\end_inset

 that turns out to have the especially nice property of 
\emph on
preserving positive-definiteness 
\emph default
(if 
\begin_inset Formula $H^{k}$
\end_inset

 is definite then 
\begin_inset Formula $H^{k+1}$
\end_inset

 is also, as we discuss below), is to choose some 
\begin_inset Formula $W$
\end_inset

 so that 
\begin_inset Formula $W^{-1}y=\delta$
\end_inset

.
 For example, we can choose 
\begin_inset Formula $W^{-1}=(H^{k+1})^{-1}=E+(H^{k})^{-1}$
\end_inset

.
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
This may seem a bit circular: we choose 
\begin_inset Formula $W$
\end_inset

 based on the 
\emph on
result
\emph default
 of the optimization.
 One way to think of it is that if you choose 
\begin_inset Formula $W$
\end_inset

 based on the 
\begin_inset Formula $E=\cdots$
\end_inset

 formula, then hold 
\begin_inset Formula $W$
\end_inset

 fix and minimize 
\begin_inset Formula $\Vert E\Vert_{W}$
\end_inset

 in our QP, you recover the same 
\begin_inset Formula $W$
\end_inset

.
\end_layout

\end_inset

 We then obtain, after a bit more algebra, the famous 
\series bold
BFGS update
\series default
: 
\begin_inset Formula 
\[
\boxed{(H^{k+1})^{-1}=(H^{k})^{-1}-\frac{1}{\gamma^{T}\delta}\left[(H^{k})^{-1}\gamma\delta^{T}+\delta\gamma^{T}(H^{k})^{-1}-\left(1+\frac{\gamma^{T}(H^{k})^{-1}\gamma}{\gamma^{T}\delta}\right)\delta\delta^{T}\right]}.
\]

\end_inset

This may look a little messy.
 Equivalently, via the Sherman–Morrison formula,
\begin_inset Foot
status open

\begin_layout Plain Layout
The Sherman–Morrison formula 
\begin_inset Formula $(A+uv^{T})^{-1}=A^{-1}-\frac{A^{-1}uv^{T}A^{-1}}{1+v^{T}A^{-1}u}$
\end_inset

 shows that a rank-1 update of 
\begin_inset Formula $A$
\end_inset

 corresponds to a rank-1 update of 
\begin_inset Formula $A^{-1}$
\end_inset

  and vice-versa.
\end_layout

\end_inset

 we can derive (after a bunch more algebra) the corresponding update of
 
\begin_inset Formula $H^{k}$
\end_inset

:
\begin_inset Formula 
\[
\boxed{H^{k+1}=H^{k}+\frac{\gamma\gamma^{T}}{\gamma^{T}\delta}-\frac{H^{k}\delta\delta^{T}H^{k}}{\delta^{T}H^{k}\delta}},
\]

\end_inset

which is easier to analyze, even though in practice it is 
\begin_inset Formula $H^{-1}$
\end_inset

 that we compute and store.
\end_layout

\begin_layout Subsection
Positive-definiteness
\end_layout

\begin_layout Standard
A key property of the choice of weight 
\begin_inset Formula $W$
\end_inset

 in the BFGS update is that it allows us to ensure positive-definiteness
 of 
\begin_inset Formula $H^{k+1}$
\end_inset

 assuming 
\begin_inset Formula $H^{k}$
\end_inset

 is definite.
 (Typically the algorithm starts with 
\begin_inset Formula $H^{0}=I$
\end_inset

 or a similar diagonal positive-definite matrix.) We simply need to check
 that 
\begin_inset Formula $x^{T}H^{k+1}x>0$
\end_inset

 for any 
\begin_inset Formula $x\ne0$
\end_inset

:
\begin_inset Formula 
\begin{align*}
x^{T}H^{k+1}x & =x^{T}H^{k}x-\frac{(\delta^{T}H^{k}x)^{2}}{\delta^{T}H^{k}\delta}+\frac{(x^{T}\gamma)^{2}}{\gamma^{T}\delta}\\
 & =\underbrace{\frac{(x^{T}H^{k}x)(\delta^{T}H^{k}\delta)-(\delta^{T}H^{k}x)^{2}}{\delta^{T}H^{k}\delta}}_{\ge0\text{ by Cauchy–Schwarz}}+\underbrace{\frac{(x^{T}\gamma)^{2}}{\gamma^{T}\delta}}_{>0\text{ if }\gamma^{T}\delta>0}.
\end{align*}

\end_inset

The first term is 
\begin_inset Formula $\ge0$
\end_inset

 by the Cauchy-Schwarz inequality: for any inner product 
\begin_inset Formula $\langle x,y\rangle$
\end_inset

, it is always true that 
\begin_inset Formula $\langle x,x\rangle\langle\delta,\delta\rangle\ge|\langle x,y\rangle|^{2}$
\end_inset

, and in this case because 
\begin_inset Formula $H^{k}$
\end_inset

 is positive-definite we have an inner product 
\begin_inset Formula $\langle x,y\rangle=x^{T}H^{k}y$
\end_inset

.
\end_layout

\begin_layout Standard
The second term is clearly 
\begin_inset Formula $>0$
\end_inset

 whenever 
\begin_inset Formula $\gamma^{T}\delta=\delta^{T}\gamma=\delta^{T}g^{k+1}-\delta^{T}g^{k}>0$
\end_inset

, but why should this be? If we did an 
\emph on
exact
\emph default
 line search, then 
\begin_inset Formula $\delta^{T}g^{k+1}=0$
\end_inset

, and 
\begin_inset Formula $-\delta^{T}g^{k}=(x^{k+1}-x^{k})^{T}(-g^{k})>0$
\end_inset

 because 
\begin_inset Formula $-g^{k}$
\end_inset

 is the 
\begin_inset Quotes eld
\end_inset

downhill
\begin_inset Quotes erd
\end_inset

 direction and 
\begin_inset Formula $x^{k+1}$
\end_inset

 must be 
\begin_inset Quotes eld
\end_inset

downhill
\begin_inset Quotes erd
\end_inset

 from 
\begin_inset Formula $x^{k}$
\end_inset

.
 If we did an 
\emph on
inexact
\emph default
 line search, but we imposed the second Wolfe condition 
\begin_inset Formula $|\delta^{T}g^{k+1}|<|\delta^{T}g^{k}|$
\end_inset

, then we still have 
\begin_inset Formula $\delta^{T}g^{k+1}-\delta^{T}g^{k}>0$
\end_inset

 (the second term is positive and the first term can't be a larger negative
 magnitude).
 If we didn't impose the second Wolfe condition and happened to do a step
 where 
\begin_inset Formula $\delta^{T}\gamma\lesssim0$
\end_inset

, then we can just skip the update: let 
\begin_inset Formula $H^{k+1}=H^{k}$
\end_inset

: violating the second Wolfe condition generally means that we took too
 small a step, and we want to keep going in the same direction.
\end_layout

\begin_layout Section
Low-storage quasi-Newton (L-BFGS)
\end_layout

\begin_layout Standard
Applying the BFGS update directly requires 
\begin_inset Formula $\Theta(n^{2})$
\end_inset

 storage for 
\begin_inset Formula $(H^{k})^{-1}$
\end_inset

 and 
\begin_inset Formula $\Theta(n^{2})$
\end_inset

 work on each step to update 
\begin_inset Formula $H^{k+1}$
\end_inset

.
 This is fine for 
\begin_inset Formula $n$
\end_inset

 up to a few thousand, but for truly large-scale optimization problems it
 is prohibitive.
 Fortunately, the fact that BFGS is made of rank-1 updates (adding rank-1
 matrices 
\begin_inset Formula $uv^{T}$
\end_inset

 to 
\begin_inset Formula $H^{k}$
\end_inset

 or its inverse), there is a solution: store a set of rank-1 updates, not
 the matrix.
 That is, represent 
\begin_inset Formula 
\[
(H^{k})^{-1}\approx H^{0}+\sum_{j=1}^{m}u^{j}(v^{j})^{T}
\]

\end_inset

 where we keep the 
\begin_inset Formula $m$
\end_inset

 most recent rank-1 updates for some 
\begin_inset Formula $m$
\end_inset

 (typically 
\begin_inset Formula $10\lesssim m\le100$
\end_inset

).
 This is known as an 
\series bold
L-BFGS
\series default
 method, where 
\begin_inset Quotes eld
\end_inset

L
\begin_inset Quotes erd
\end_inset

 stands for 
\begin_inset Quotes eld
\end_inset

low-storage
\begin_inset Quotes erd
\end_inset

, and was introduced by Nocedal in
\begin_inset space ~
\end_inset

1980
\begin_inset space ~
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "Byrd94"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
With this representation of 
\begin_inset Formula $(H^{k})^{-1}$
\end_inset

, assuming 
\begin_inset Formula $H^{0}$
\end_inset

 is sparse (typically diagonal, e.g.
 
\begin_inset Formula $I$
\end_inset

), the storage cost is 
\begin_inset Formula $\Theta(mn)$
\end_inset

 for the 
\begin_inset Formula $\{u^{j},v^{j}\}$
\end_inset

 vectors, the cost to multiply 
\begin_inset Formula $(H^{k})^{-1}g^{k}$
\end_inset

 for the quasi-Newton step is also 
\begin_inset Formula $\Theta(mn)$
\end_inset

, where as usual we compute 
\begin_inset Formula $uv^{T}g$
\end_inset

 by 
\begin_inset Formula $u(v^{T}g)$
\end_inset

 in 
\begin_inset Formula $\Theta(n)$
\end_inset

 operations, and the cost of updating to 
\begin_inset Formula $H^{k+1}$
\end_inset

 is 
\begin_inset Formula $\Theta(mn)$
\end_inset

 for the 
\begin_inset Formula $(H^{k})^{-1}\gamma$
\end_inset

 product plus 
\begin_inset Formula $\Theta(n)$
\end_inset

 other operations.
\end_layout

\begin_layout Standard
Although for 
\begin_inset Formula $m\ll n$
\end_inset

 this procedure can no longer converge to the exact Hessian, in practice
 L-BFGS can greatly accelerate optimization (compared to steepest-descent
 and other first-order methods with 
\begin_inset Quotes eld
\end_inset

no memory
\begin_inset Quotes erd
\end_inset

) in many cases, especially optimization to high accuracy, in much the same
 way as an approximate Krylov method like restarted GMRES or nonlinear conjugate
-gradient.
\end_layout

\begin_layout Section
BFGS and constrained optimization
\end_layout

\begin_layout Standard
For nonlinearly constrained optimization (
\begin_inset Formula $\min f_{0}(x)$
\end_inset

 subject to 
\begin_inset Formula $f_{i}(x)\le0$
\end_inset

), the most common utilization of BFGS has been for sequential quadratic
 programming (SQP): approximate the optimization problem by a sequence of
 convex QP (convex quadratic objective + affine constraints), typically
 solved in a trust region to give each optimization step.
 BFGS is then used to obtain the quadratic term in the QP, but there are
 a variety of ways to do this.
 The simplest is to apply BFGS to 
\begin_inset Formula $f_{0}$
\end_inset

 , but in that case only linear approximations are used for the constraints
 
\begin_inset Formula $f_{i}$
\end_inset

.
 Alternatively, BFGS can be applied to some form of Lagrangian or 
\begin_inset Quotes eld
\end_inset

augmented
\begin_inset Quotes erd
\end_inset

 Lagrangian (=
\begin_inset space ~
\end_inset

Lagrangian +
\begin_inset space ~
\end_inset

penalties for violated constraints)
\begin_inset space ~
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "Byrd92"
literal "false"

\end_inset

.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "Broyden70"
literal "false"

\end_inset

C.
 Broyden, 
\begin_inset Quotes eld
\end_inset

The convergence of a class of double-rank minimization algorithms,
\begin_inset Quotes erd
\end_inset

 
\emph on
J.
 Inst.
 Math.
 Appl.

\series bold
\emph default
 6
\series default
, pp.
 76–90 (1970).
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "Fletcher70"
literal "false"

\end_inset

R.
 Fletcher, 
\begin_inset Quotes eld
\end_inset

A new approach to variable-metric algorithms,
\begin_inset Quotes erd
\end_inset

 
\emph on
Computer J.

\emph default
 
\series bold
13
\series default
, pp.
 317–322 (1970).
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "Goldfarb70"
literal "false"

\end_inset

D.
 Goldfarb, 
\begin_inset Quotes eld
\end_inset

A family of variable-metric methods derived by variational means,
\begin_inset Quotes erd
\end_inset

 
\emph on
Math.
 Comp.

\emph default
 
\series bold
24
\series default
, pp.
 23–26 (1970).
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "Shanno70"
literal "false"

\end_inset

D.
 Shanno, 
\begin_inset Quotes eld
\end_inset

Conditioning of quasi-Newton methods for function minimization,
\begin_inset Quotes erd
\end_inset

 
\emph on
Math.
 Comp.

\emph default
 
\series bold
24
\series default
, pp.
 647–656 (1970).
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "FletcherPowell63"
literal "false"

\end_inset

R.
 Fletcher and M.
 J.
 D.
 Powell, 
\begin_inset Quotes eld
\end_inset

A rapidly convergent descent method for minimization,
\begin_inset Quotes erd
\end_inset

 
\emph on
Comput.
 J.

\emph default
 
\series bold
6
\series default
, pp.
 163–168 (1963).
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "Broyden67"
literal "false"

\end_inset

C.
 G.
 Broyden, 
\begin_inset Quotes eld
\end_inset

Quasi-Newton methods and their application to function minimisation,
\begin_inset Quotes erd
\end_inset

 
\emph on
Math.
 Comp.

\emph default
 
\series bold
21
\series default
, pp.
 368–381 (1967).
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "Greenstadt70"
literal "false"

\end_inset

J.
 Greenstadt, 
\begin_inset Quotes eld
\end_inset

Variations on variable metric methods,
\begin_inset Quotes erd
\end_inset

 
\emph on
Math.
 Comp.

\emph default
 
\series bold
24
\series default
, pp.
 1–22 (1970).
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "Byrd94"
literal "false"

\end_inset

R.
 H.
 Byrd, J.
 Nocedal, R.
 B.
 Schnabel, 
\begin_inset Quotes eld
\end_inset

Representations of quasi-Newton matrices and their use in limited memory
 methods,
\begin_inset Quotes erd
\end_inset

 
\emph on
Math.
 Prog.

\emph default
 
\series bold
63
\series default
, pp.
 129–156 (1994).
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "Byrd92"
literal "false"

\end_inset

R.
 H.
 Byrd, R.
 A.
 Tapia, Y.
 Zhang, 
\begin_inset Quotes eld
\end_inset

An SQP augmented Lagrangian BFGS algorithm for constrained optimization,
\begin_inset Quotes erd
\end_inset

 
\emph on
SIAM J.
 Optim.

\emph default
 
\series bold
2
\series default
, pp.
 210–241 (1992).
\end_layout

\end_body
\end_document
